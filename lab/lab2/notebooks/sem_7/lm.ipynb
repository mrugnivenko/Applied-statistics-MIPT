{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Языковая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим языковую модель сначала вручную на простом синтетическом корпусе, затем обучим модель из пакета `nltk` на стихотворении \"Дом, который построил Джек\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import nltk.lm as lm\n",
    "from nltk.util import ngrams as nltk_ngrams\n",
    "import numpy as np\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'SOS SOS ' + 'А Б ' * 100 + 'EOS'\n",
    "tokens = text.split()\n",
    "n = len(tokens)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_and_prefix_counts(tokens, n_max):\n",
    "    # словарь n-грамм и их частот\n",
    "    ngrams_counts = {}\n",
    "    # словарь n-граммных префиксов и их частот\n",
    "    prefix_counts = {}\n",
    "    \n",
    "    n = len(tokens)\n",
    "    for i in range(n_max):\n",
    "        ngrams_counts[i + 1] = Counter([tuple(tokens[j : j + i + 1]) for j in range(n - i)])\n",
    "        prefix_counts[i + 1] = Counter([tuple(tokens[j : j + i] + ['*']) for j in range(n - i)])\n",
    "\n",
    "    return ngrams_counts, prefix_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts, prefix_counts = ngrams_and_prefix_counts(tokens, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-граммы и их частотные вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat p_i = \\hat p(w_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_probas(ngram_counts):\n",
    "    p1 = {}\n",
    "    n = sum(ngram_counts[1].values())\n",
    "    for w in ngram_counts[1]:\n",
    "        p1[w] = ngram_counts[1][w] / n\n",
    "    return p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = unigram_probas(ngram_counts)\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat p_{i, i - 1} = \\hat p(w_i|w_{i - 1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_probas(ngram_counts, prefix_counts):\n",
    "    p2 = {}\n",
    "    for w in ngram_counts[2]:\n",
    "        pre_w = tuple([w[0]] + ['*'])\n",
    "        p2[u'{1}|{0}'.format(*w)] = ngram_counts[2][w] / prefix_counts[2][pre_w]\n",
    "    return p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = bigram_probas(ngram_counts, prefix_counts)\n",
    "p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat p_{i, i - 1, i - 2} = \\hat p(w_i|w_{i - 1}, w_{i - 2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_probas(ngram_counts, prefix_counts):\n",
    "    p3 = {}\n",
    "    for w in ngram_counts[3]:\n",
    "        pre_w = w[:2] + tuple(['*'])\n",
    "        p3[u'{2}|{1},{0}'.format(*w)] = ngram_counts[3][w] / prefix_counts[3][pre_w]\n",
    "    return p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 =  trigram_probas(ngram_counts, prefix_counts)\n",
    "p3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка гипотезы, что триграммную модель можно свести к биграммной против правосторонней альтернативы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистика:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-2 \\log (\\prod_{i, j, k = 1}^m (\\hat p_{ij} / \\hat p_{ijk})^{n_{ijk}}) = \\sum_{i, j, k}^m -2 n_{ijk} \\log \\hat p_{ij} + 2 n_{ijk} \\log \\hat p_{ijk} = \\sum_{i = 3}^N -2 \\log \\hat p_{i,i - 1} + 2 \\log \\hat p_{i, i - 1, i - 2},$$\n",
    "$$n_{ijk} = |\\{X_t: X_t = O_i, X_{t + 1} = O_j, X_{t + 2} = O_k\\}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_statistic(p2, p3, tokens):\n",
    "    stat2 = []\n",
    "    stat3 = []\n",
    "    n = len(tokens)\n",
    "    for i in range(n - 2):\n",
    "        w = tokens[i : i + 3]\n",
    "        ngram3 = '{2}|{1},{0}'.format(*w)\n",
    "        ngram2 = '{1}|{0}'.format(*w)\n",
    "\n",
    "        stat2.append(np.log(p2[ngram2]))\n",
    "        stat3.append(np.log(p3[ngram3]))\n",
    "    return - 2 * np.sum(stat2) + 2 * np.sum(stat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(p3)\n",
    "stat = chi2_statistic(p2, p3, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p-value = {1 - st.distributions.chi2(m * ((m - 1) ** 2) - 1).cdf(stat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гипотеза не отвергается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другой пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'SOS SOS ' + 'А Б Б А Б А Б А Б Б А А ' * 100\n",
    "tokens = text.split()\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts, prefix_counts = ngrams_and_prefix_counts(tokens, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = unigram_probas(ngram_counts)\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = bigram_probas(ngram_counts, prefix_counts)\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 =  trigram_probas(ngram_counts, prefix_counts)\n",
    "p3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка той же гипотезы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = chi2_statistic(p2, p3, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p-value = {1 - st.distributions.chi2(m * ((m - 1) ** 2) - 1).cdf(stat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гипотеза отвергается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сглаживание Лапласа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = list(nltk_ngrams(tokens, 1))\n",
    "n2 = list(nltk_ngrams(tokens, 2))\n",
    "n3 = list(nltk_ngrams(tokens, 3))\n",
    "n3[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace = lm.Laplace(order=3)\n",
    "laplace.fit([n1] + [n2] + [n3], vocabulary_text=list(set(tokens)))\n",
    "regular_lm = lm.MLE(order=3)\n",
    "regular_lm.fit([n1] + [n2] + [n3], vocabulary_text=list(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перплексия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Меньше => лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace.perplexity(n1), regular_lm.perplexity(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = [('b'), ('a'), ('r')]\n",
    "laplace.perplexity(foo), regular_lm.perplexity(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сглаженная по Лапласу оценка вероятности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_L(w_i) = \\frac{c_i + 1}{\\sum_{i = 1}^v c_i + v}$$\n",
    "$$p_L(w_i|w_j) = \\frac{c_{ij} + 1}{\\sum_{j=1}^v (c_{ij} + 1)} = \\frac{c_{ij} + 1}{c_i + v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_L('А'|'SOS')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace.score('А', context=['SOS']), regular_lm.score('А', context=['SOS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_L('SOS')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace.score('SOS'), regular_lm.score('SOS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти n-граммы не встречались в тексте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace.score('C', context=['SOS']), laplace.score('ыаываа', context=['B']), laplace.score('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_lm.score('C', context=['SOS']), regular_lm.score('ыаываа', context=['B']), regular_lm.score('B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Дом, который построил Джек\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "rt = RegexpTokenizer(u'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jack.txt') as f:\n",
    "    text = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = rt.tokenize(text)\n",
    "len(tokens), len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = list(nltk_ngrams(tokens, 1) )\n",
    "n2 = list(nltk_ngrams(tokens, 2))\n",
    "n3 = list(nltk_ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplace = lm.Laplace(order=3)\n",
    "laplace.fit([n1] + [n2] + [n3], vocabulary_text=list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(laplace.generate(50, random_seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(laplace.generate(50, text_seed='вот дом который построил джек'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(laplace.generate(50, text_seed='привет как дела'.split()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 (pyenv)",
   "language": "python",
   "name": "pyenv3.6.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
